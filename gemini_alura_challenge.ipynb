{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyPnrFgEahi7ykzNNg8rtW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/synkodev/gemini-alura-challenge/blob/main/gemini_alura_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Using Google's Generative AI to recommend dishes based on user input"
      ],
      "metadata": {
        "id": "3H7J6QDcHiKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apresenta√ß√£o do projeto\n",
        "\n",
        "üëã Seja bem-vindo ao meu projeto realizado com os conhecimentos adquiridos na semana de Imers√£o IA Alura + Google. A inten√ß√£o deste projeto √© de colocar em pr√°tica os conhecimentos adquiridos durante essa semana de imers√£o, na qual aprendemos o que √© o Gemini, como efetivamente aplicar t√©cnicas eficazes de prompting, o b√°sico da utiliza√ß√£o da SDK do Python e os conceitos por tr√°s da IA Generativa.\n",
        "\n",
        "üìò O projeto consiste em utilizar a IA para fazer recomenda√ß√µes de pratos e responder outras quest√µes do usu√°rio com base em menus dispon√≠veis e pr√©-determinados. Isso poderia ser muito √∫til considerando aplicativos de entrega de comida, por exemplo, no qual o usu√°rio poderia selecionar alguns restaurantes e fazer perguntas com base nas op√ß√µes fornecidas pelos mesmos. Para isso, utilizarei o modelo de embedding para \"varrer\" os menus informados, encontrando o trecho do documento que mais se adequa √† busca do usu√°rio. Na sequ√™ncia, utilizarei o modelo Generativo para gerar uma resposta adequada ao usu√°rio. Para este fim, utilizarei o Gemini 1.5 Pro.\n",
        "\n",
        "Espero que voc√™ goste deste projeto e me diga o que voc√™ achou no meu [LinkedIn](https://www.linkedin.com/in/walencar) ou no meu [GitHub](https://www.github.com/synkodev). Obrigado e boa leitura! üòä\n",
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "\n",
        "## English introduction\n",
        "\n",
        "üëã Welcome to my final project of the IA Week Alura + Google event. This project aims to practice all the knowledge I've acquired during this intensive learning week, in which we were told what is Gemini, how to correctly apply efficient prompting techniques, the basics of Python's SDK for generative AI and so on.\n",
        "\n",
        "üìò The project consists of using AI to recommend dishes or answer any inquiries from the user based on pre-determined available menus. This could be useful when applied in the context of delivery apps, where the user could possibly select a few restaurants of their choice and then ask a few questions to find a dish of their interest without having to search each menu individually. To accomplish this, I'll use the embedding model to search in the menus passages that are most relevant according to the user's input or inquiry. Once that is done, I'll use the generative AI model to generate an adequare response to the user. For this, Gemini 1.5 Pro will be used.\n",
        "\n",
        "I hope you like this project! Most texts will be in Portuguese, but the code commentary will be in English at most times. Tell me what you think about this project on my [LinkedIn](https://www.linkedin.com/in/walencar) profile or you cand find me on [GitHub](https://www.github.com/synkodev) as well. Thank you for reading! üòä"
      ],
      "metadata": {
        "id": "0AFQb0dCHz3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pr√©-requisitos de execu√ß√£o"
      ],
      "metadata": {
        "id": "6Bc-MUkWMP5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiramente, precisamos instalar a SDK para Python da IA do Google. O Colab j√° possui algumas bibliotecas instaladas no ambiente por padr√£o, e que portanto n√£o precisam ser *instaladas*, apenas mencionadas no passo de importa√ß√£o (a seguir)."
      ],
      "metadata": {
        "id": "B4zGVehijHSR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WI-HKWzii-SG"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai # Install the Python SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma vez instaladas todas as nossas depend√™ncias, podemos prosseguir com a importa√ß√£o da biblioteca rec√©m-instalada e das demais que necessitaremos para trabalhar com nossos dados. Nomearemos um *alias* que facilite a chamada dentro do c√≥digo como **genai**.\n",
        "\n",
        "*   `textwrap`: para formata√ß√£o de texto.\n",
        "*   `numpy`: para opera√ß√µes matem√°ticas avan√ßadas.\n",
        "*   `pandas`: para utilizarmos a estrutura de DataFrame.\n",
        "*   `google.generativeai`: nossa IA do Google\n",
        "*   `userdata`: para \"puxarmos\" nossa chave de API do cofre do Colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "6DcDON4ljeYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap # Text formatting\n",
        "\n",
        "import numpy as np # Numpy for math operations\n",
        "import pandas as pd # Pandas for exploring DataFrames\n",
        "\n",
        "import google.generativeai as genai # Imports the library to work with AI\n",
        "from google.colab import userdata # Allows to keep secret keys @ Colab Secrets"
      ],
      "metadata": {
        "id": "UuKIXUiqja0_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Primeiros passos\n",
        "Come√ßaremos utilizando nossa chave de API do AI Studio atrav√©is da Secret salva no Colab. A chave da API √© necess√°ria para configurar a inicializa√ß√£o do modelo, que mais tarde instanciaremos atrav√©s do m√©todo `genai.GenerativeModel`.\n",
        "\n",
        "Uma vez inserida nossa chave, pediremos uma lista de modelos dispon√≠veis que sejam de gera√ß√£o de conte√∫do e *embedding*. Utilizaremos o nome dos modelos ao inicializar um novo modelo."
      ],
      "metadata": {
        "id": "LzQKd0HSkqm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gets the secret key kept @ Colab\n",
        "api_key = userdata.get(\"gemini\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Iteraction to show every name for embedding and generative models\n",
        "for m in genai.list_models():\n",
        "  if \"generateContent\" in m.supported_generation_methods or \"embedContent\" in m.supported_generation_methods:\n",
        "    print(m.name);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "_IyaO4-QkqOV",
        "outputId": "13ff185f-a82f-4b51-c713-bffeeca31e56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfeito! Com base nos nomes dispon√≠veis acima, separaremos em duas vari√°veis quais modelos utilizaremos para a gera√ß√£o da resposta no final do projeto e para varrer o documento em busca de trechos que tenham maior rela√ß√£o com a pergunta feita pelo usu√°rio.\n",
        "\n",
        "Por hora, criaremos um modelo generativo utilizando o **Gemini 1.5 Pro**, apontando para a √∫ltima vers√£o dispon√≠vel para este modelo."
      ],
      "metadata": {
        "id": "PhFIDTidOAXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create variable with strings for each model we'll use\n",
        "generative_model = \"gemini-1.5-pro-latest\"\n",
        "embedding_model = \"models/embedding-001\"\n",
        "\n",
        "# Instantiate a new generative model pointing to the Gemini 1.5 Pro, later used for prompting\n",
        "model = genai.GenerativeModel(model_name=generative_model)"
      ],
      "metadata": {
        "id": "WDkfXWf9rlPC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando os dados utilizados\n",
        "\n",
        "A seguir, informaremos quais os documentos que ser√£o levados em considera√ß√£o para realizar o embedding. Isto √©, dado o input do usu√°rio, o modelo ir√° passar pelo conte√∫do destes tr√™s documentos que escreveremos abaixo, simulando menu de diferentes restaurantes, e encontrar√° qual o menu que atende melhor ao que foi perguntando pelo usu√°rio.\n",
        "\n",
        "Para a cria√ß√£o dos documentos utilizaremos duas chaves. `\"Restaurant\"` armazenar√° o nome do restaurante, enquanto `\"Menu\"` salvar√° todas as informa√ß√µes relacionadas aos pratos, como nome, pre√ßo, e os ingredientes que cada um cont√©m.\n",
        "\n",
        "Por fim, salvaremos os tr√™s documentos, todos com o mesmo formato de chave-valor, na vari√°vel `documents` que ser√° usada para realizar o *embedding*."
      ],
      "metadata": {
        "id": "fuhskTFaOiGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating three documents for embedding\n",
        "DOCUMENT1 = {\n",
        "    # First key: Restaurant, stores the restaurant name\n",
        "    \"Restaurant\": \"Chef's Kiss\",\n",
        "    # Second key: Menu, stores all info regarding the dish name, price and ingredients, separated by this '---' structure\n",
        "    \"Menu\": \"\"\"\n",
        "    Salmon Crab Cakes\n",
        "\n",
        "    13,90 euros\n",
        "\n",
        "    fresh blue crab\n",
        "    salmon\n",
        "    sriracha\n",
        "    curried mango\n",
        "    pineapple chutney\n",
        "    ---\n",
        "    Boneless Beef Short Rib\n",
        "\n",
        "    12 euros\n",
        "\n",
        "    piece of rib\n",
        "    beer\n",
        "    cherries\n",
        "    onoins\n",
        "    parnsip puree\n",
        "    spaetzle\n",
        "    seasonal vegetables\n",
        "    ---\n",
        "    Chicken Supreme\n",
        "\n",
        "    9,90 euros\n",
        "\n",
        "    chicken\n",
        "    wild mushrooms\n",
        "    caramelized onions\n",
        "    garlic whipped potatoes\n",
        "    seasonal vegetables\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "DOCUMENT2 = {\n",
        "    \"Restaurant\": \"Veggie Maggie\",\n",
        "    \"Menu\": \"\"\"\n",
        "    BUTTERNUT SQUASH MALAYSIAN CURRY\n",
        "\n",
        "    15,95 euros\n",
        "\n",
        "    jackfruit\n",
        "    pumpkin\n",
        "    mushrooms\n",
        "    turmeric\n",
        "    curry sauce\n",
        "    brown basmati\n",
        "    rice\n",
        "    peanuts\n",
        "    coriander\n",
        "    ---\n",
        "    RAW VEGAN LASAGNA\n",
        "\n",
        "    14,35 euros\n",
        "\n",
        "    raw zucchini\n",
        "    fresh tomatoes and dried tomatoes sauce\n",
        "    Goji berries\n",
        "    cashews and macadamia nuts 'cheese-like' cream\n",
        "    pico de gallo\n",
        "    ---\n",
        "    PLANT-BASED TRUFFLE MAYO BURGER\n",
        "\n",
        "    12,95 euros\n",
        "\n",
        "    whole spelt gluten-free brioche bread\n",
        "    plant-based hamburger\n",
        "    truffled mayonnaise\n",
        "    saut√©ed mushrooms\n",
        "    roasted onion\n",
        "    plant-based 'cheddar'\n",
        "    roasted sweet potatoes with plant-based yogurt sauce\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "DOCUMENT3 = {\n",
        "    \"Restaurant\": \"McClown's\",\n",
        "    \"Menu\": \"\"\"\n",
        "    Big Clown\n",
        "\n",
        "    7 euros\n",
        "\n",
        "    90g Hamburguer\n",
        "    Lettuce\n",
        "    Tomatoes\n",
        "    Mayonnaise\n",
        "    Picles\n",
        "    Cheddar\n",
        "    ---\n",
        "    Fish Clown Burguer\n",
        "\n",
        "    6 euros\n",
        "\n",
        "    Fish hamburguer\n",
        "    Lettuce\n",
        "    Tomatoes\n",
        "    Mayonnaise\n",
        "    Cheddar\n",
        "    Lemon-based sauce dip\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
      ],
      "metadata": {
        "id": "yGd8-oOY2yYq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para facilitar a manipula√ß√£o dos documentos, converteremos esses documentos todos em uma estrutura de dataframe."
      ],
      "metadata": {
        "id": "YOniA1ky7twN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the documents created above to a DF structure to further manipulate it easily\n",
        "df = pd.DataFrame(documents)\n",
        "df.columns = [\"Restaurant\", \"Menu\"]\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZLLpwf87w-L",
        "outputId": "83a51bcc-0be3-46c5-8a34-f7d386012849"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Restaurant                                               Menu\n",
            "0    Chef's Kiss  \\n    Salmon Crab Cakes\\n\\n    13,90 euros\\n\\n...\n",
            "1  Veggie Maggie  \\n    BUTTERNUT SQUASH MALAYSIAN CURRY\\n\\n    ...\n",
            "2      McClown's  \\n    Big Clown\\n\\n    7 euros\\n\\n    90g Hamb...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acima, podemos ver a representa√ß√£o dos dados contidos na chave `\"Restaurant\"` na primeira coluna, e os dados da chave `\"Menu\"` na segunda. Al√©m disso, cada documento que criamos se tornou uma linha (row) do DataFrame `df`."
      ],
      "metadata": {
        "id": "ErsADCNsQHOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n",
        "\n",
        "Agora, para realizarmos a nossa busca com base no input do usu√°rio e facilitar a aproxima√ß√£o sem√¢ntica entre cada documento, vamos transformar o conte√∫do de cada documento em algo que seja mais pr√≥ximo do que o nosso algoritmo efetivamente entende. E √© justamente isso que √© o *embedding*: a convers√£o desses textos em uma estrutura de vetores num√©ricos que pode ser processada por algoritmos de ML, e essa estrutura √© projetada **para capturar o valor sem√¢ntico e o contexto que cada \"palavra\" representa**.\n",
        "\n",
        "Precisamos gerar esse vetor para cada item do nosso DataFrame."
      ],
      "metadata": {
        "id": "n1SxYnRXQc-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function will be applied to each row of `df` and create a new column containing the embedded value of each row\n",
        "# Embedding will turn all the text content in a numerical vector representation, projected to identify context and semantical value\n",
        "def embed_fn(restaurant, menu):\n",
        "  return genai.embed_content(model=embedding_model,\n",
        "                             content=menu,\n",
        "                             task_type=\"retrieval_document\",\n",
        "                             title=restaurant)[\"embedding\"]"
      ],
      "metadata": {
        "id": "nn6XSh-R8Qu8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executando a fun√ß√£o acima para cada linha de `df`, geramos uma coluna `\"Embeddings\"` que cont√©m a sua representa√ß√£o vetorizada."
      ],
      "metadata": {
        "id": "w_LbvzmoRm4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 'Embeddings' column by applying `embed_fn`\n",
        "df['Embeddings'] = df.apply(lambda row: embed_fn(row['Restaurant'], row['Menu']), axis=1)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "OXWIL1MM8hpl",
        "outputId": "0288b6e3-a5fd-4de0-b3c7-0c715226b226"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Restaurant                                               Menu  \\\n",
            "0    Chef's Kiss  \\n    Salmon Crab Cakes\\n\\n    13,90 euros\\n\\n...   \n",
            "1  Veggie Maggie  \\n    BUTTERNUT SQUASH MALAYSIAN CURRY\\n\\n    ...   \n",
            "2      McClown's  \\n    Big Clown\\n\\n    7 euros\\n\\n    90g Hamb...   \n",
            "\n",
            "                                          Embeddings  \n",
            "0  [0.050908875, -0.009233177, 0.002469926, -0.04...  \n",
            "1  [0.003577931, -0.042687457, 0.00065681455, -0....  \n",
            "2  [0.0123029705, -0.043154325, -0.030352496, -0....  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recebendo input de usu√°rio\n",
        "\n",
        "Agora, pediremos um input de usu√°rio. Pode ser uma pergunta, do tipo \"Qual o restaurante com mais op√ß√µes veganas?\" ou ainda uma ordem do tipo \"Sugira um prato com alto teor cal√≥rico\"."
      ],
      "metadata": {
        "id": "bkeHpIxTSkvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Requests for user input\n",
        "user_input = input(\"Enter question or prompt: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jlp_FWjB8z3",
        "outputId": "e2a97ec8-fbf2-4067-a0b9-0212d84d88dc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter question or prompt: lalaland\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assim como fizemos com os dados do nosso documento, tamb√©m precisamos transformar o que o nosso usu√°rio nos passou para uma estrutura de dados que seja mais familiar para o algoritmo de aprendizado de m√°quina. Vamos gerar o embedding desse input de usu√°rio."
      ],
      "metadata": {
        "id": "WGqLMqNnT8a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Also turns the user input into embedded information, so it can be compared to the semantic value of each row of our `df`\n",
        "request = genai.embed_content(model=embedding_model,\n",
        "                              content=user_input,\n",
        "                              task_type=\"retrieval_query\")\n",
        "print(request)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "fiuL3b23880t",
        "outputId": "ad0fa1a9-fb41-4922-c0eb-455b30623e65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'embedding': [0.019074757, -0.056308046, -0.029934365, 0.041971743, 0.027336726, -0.0061310343, 0.014299876, -0.015082191, 0.024205603, 0.04964768, 0.015723864, -0.013006708, -0.026740521, -0.040987577, -0.0068481066, 0.016006256, -0.032048646, -0.0034223504, 0.030954586, -0.045143455, 0.0037859464, -0.000709151, -0.03530885, -0.022025129, -0.0067549367, 0.005427041, 0.0067314343, -0.047103796, -0.01379762, 0.0134636145, -0.09312128, 0.06791194, -0.0718045, -0.022353198, -0.023214562, -0.08256519, 0.0011906382, 0.052671757, 0.035843957, 0.060302105, 0.0072748912, -0.007934607, -0.05497862, -0.0041299444, 0.019347496, -0.07355151, -0.025041033, 0.010869258, -0.01641778, -0.07251568, -0.008359695, -0.026266156, 0.05305042, 0.0007017568, -0.032303378, -0.088730074, 0.04033678, 0.03214192, -0.022957103, 0.036769334, -0.017328735, 0.026666777, -0.010661408, 0.029052706, 0.040886257, -0.016199635, -0.0008797388, -0.0074425056, 0.045768443, 0.0032213451, 0.0694455, -0.057440437, 0.012748921, 0.0039298534, -0.046639744, -0.08935833, -0.03443603, 0.079842545, 0.008460447, 0.0056897607, 0.0021487065, -0.063241154, -0.03228432, -0.042456068, -0.031416103, 0.081102856, -0.08804155, -0.011200668, -0.005217621, 0.04706065, -0.017687103, 0.010081886, 0.035509396, -0.025669828, -0.011231783, 0.08035633, -0.04316971, -0.028160952, -0.032554187, -0.030441057, 0.017486611, -0.032706648, -0.098212816, -0.032988857, 0.021881083, 0.042072233, -0.0015056181, 0.038075652, -0.03632869, 0.067389406, -0.04846044, -0.00544643, -0.00031537432, -0.029986383, 0.016383363, -0.0020211136, -0.0034010303, 0.048559487, 0.054417677, 0.0309458, -0.0075693745, 0.019882413, 0.045652326, 0.009982339, 0.0404166, 0.00048381014, 0.022122422, 0.016535966, 0.039299775, 0.024597662, -0.012907386, -0.029475367, -0.043070115, 0.042863924, 0.026577191, 0.049009353, 0.016325602, -0.0014704185, 0.051489532, -0.016526788, -0.046783186, 0.02363707, -0.035344653, 0.045221563, 0.004172562, 0.027912866, -0.033480976, -0.014193093, 0.06935692, -0.03497527, 0.0064648497, -0.016214885, -0.0757596, 0.04706273, 0.047889452, 0.0038746968, -0.020129694, 0.029939748, 0.049278144, 0.0129965525, 0.026209014, 0.02532201, -0.010883689, -0.0033436278, -0.014813005, -0.0395561, 0.009752849, 0.031961776, 0.020709736, 0.013107305, 0.020247865, -0.0098279165, -0.02512543, -0.01646846, -0.011600191, -0.021228654, 0.03400485, 0.0025425379, 0.026828833, -0.0538058, -0.054356758, -0.04754384, 0.014732592, -0.00059091207, -0.024139643, -0.047916487, -0.007143594, -0.027284963, -0.025637588, -0.036452044, 0.026535802, -0.016157724, 0.0123001095, -0.0028663422, -0.027912596, 0.015699789, -0.00059327, 0.023821857, -0.006455958, -0.055871006, -0.019635601, 0.085466035, 0.052643497, -0.02857498, 0.048211455, 0.012659328, 0.04610256, -0.030974774, -0.011664232, 0.036469232, -0.047914255, 0.03703128, -0.07683301, 0.0018384687, 0.046312574, 0.043361273, -0.0012175103, 0.02470153, 0.021616722, -0.067242764, -0.012605553, -0.015712926, -0.045485362, 0.006343545, -0.051790554, 0.05113856, -0.010694887, 0.032445233, -0.021568555, -0.08353105, 0.04170566, 0.021158509, 0.03235892, 0.018605957, 0.048306074, -0.023317443, 0.029980315, 0.026493374, -0.0005008091, 0.0014882426, -0.016076192, -0.008967449, 0.039483793, 0.0505846, -0.037386134, -0.0535448, 0.0067549427, 0.07421461, -0.024177758, 0.018856822, 0.012578035, -0.02502454, 0.014569021, 0.027626816, -0.07787196, -0.005359965, -0.07481516, -0.007297692, -0.0057536573, 0.019946203, 0.061494894, 0.02346609, 0.024996435, 0.0008069031, -0.01627854, 0.01395596, 0.025970927, -0.037536535, 0.032399934, 0.012297764, 0.03757764, -0.03518169, 0.08444961, 0.039204188, 0.006036306, 0.04085851, -0.027177483, 0.052809097, 0.02485631, -0.04360624, -0.0063172267, 0.059386, 0.009221697, -0.0010754593, -0.015902746, -0.040965527, -0.06539491, -0.030823601, 0.055502288, -0.015287612, -0.008512808, -0.022974813, 0.022103779, -0.0052117687, -0.010871124, -0.03932531, -0.03118869, 0.006125912, 0.020683063, -0.0023523427, -0.017184323, -0.020270482, 0.017606394, -0.039704897, 0.001294794, 0.002851229, -0.04800422, -0.016387857, -0.0023028038, -0.025306525, 0.02394529, 0.023204984, -0.048755955, 0.012942917, -0.00521592, 0.0541315, -0.0545061, 0.01568831, -0.024056252, 0.05184138, 0.0049093165, 0.0773954, 0.0076132594, -0.008476489, 0.03719412, 0.0059871916, -0.027093574, -0.0010912193, -0.036110464, -0.00898405, -0.047679015, 0.014504995, -0.038252473, 0.04021809, 0.004383092, 0.014226412, -0.084881335, -0.0046107196, -0.012406582, 0.023419386, 0.054120965, -0.019235203, -0.050152928, -0.057813197, -0.026795944, -0.02183817, -0.0074969544, -0.03678042, 0.056912858, 0.07357199, -0.019590903, 0.03839027, 0.011600274, -0.016849931, 0.0017260825, -0.036372058, 0.056929056, -0.02203418, 0.023922505, 0.0042464053, -0.07066502, 0.0063184905, -0.0077385935, 0.014046382, -0.018144801, 0.00045445166, 0.024974633, 0.01964577, -0.028604623, 0.022312226, -0.036346342, -0.013838137, 0.036047354, 0.021402976, -0.03230881, -0.013704742, -0.010893677, -0.031450637, 0.008944065, -0.012375531, 0.011679575, -0.058660645, 0.016168987, 0.017994387, 0.0029426508, 0.0018247504, 0.055799767, 0.015576528, 0.026454758, 0.06213968, -0.0093894135, 0.02319957, 0.05741476, 0.024846831, -0.014099529, -0.025175113, -0.039694812, 0.01981455, 0.018178435, -0.014197659, 0.013628851, -0.047014963, -0.0383904, -0.050056998, -0.036386732, -0.04195884, -0.02506091, -0.033091854, -0.05542086, -0.042891826, -0.0196599, 0.041079946, 0.03257556, -0.041857924, -0.066245906, -0.07007116, 0.050084714, -0.0068247123, -0.0011394985, 0.016188344, 0.005222248, -0.0048118653, 0.030109916, 0.007075884, -0.055377383, -0.089224875, 0.016236747, -0.017936956, -0.009923497, 0.04347265, -0.007528693, 0.0093266405, 0.015914502, -0.03426575, 0.014321046, -0.051029816, 0.0032963401, 0.015688393, -0.020392098, -0.03810495, 0.0007734624, -0.015086582, 0.04645223, 0.052583158, -0.056124512, -0.065898746, 0.0017879738, -0.01795701, 0.06002128, -0.08724063, -0.0008651423, -0.0583103, -0.052582808, -0.056379434, -0.025263285, -0.011833005, 0.05526328, 0.04577383, -0.04547866, 0.002494408, -0.004081657, -0.06369997, 0.0005356698, -0.07036103, 0.028094733, 0.018628702, 0.06209295, 0.01035378, 0.028287552, 0.031790305, 0.0022489359, -0.043921523, 0.007977518, -0.054028194, -0.02259114, 0.0056546656, -0.08349446, -0.005085696, 0.0030542822, -0.0077086287, -0.03945863, -0.037016783, 0.03905944, 0.03151303, -0.0032791279, -0.0051468844, -0.0145073375, -0.0019006816, -0.007808422, -0.011137685, -0.014204951, -0.018959826, -0.08203757, -0.05446777, -0.05478609, 0.050906267, -0.042338528, -0.014401257, -0.016002852, -0.011256823, 0.030525614, -0.032356914, 0.0033317828, -0.015064404, 0.020440307, -0.009970451, 0.025057806, 0.04356477, -0.007330461, 0.001109731, 0.0515751, -0.0041378085, -0.0043722126, -0.029200505, 0.042004183, 0.0097830165, -0.00917359, 0.03679181, 0.012679549, 0.009314098, 0.028908545, 0.040485714, -0.08112961, 0.020309871, 0.026189078, -0.04320114, -0.012167716, 0.022819292, -0.03517333, 0.036433693, 0.023442939, 0.06713455, -0.055536624, -0.017850846, 0.015611886, 0.022240987, 0.03934356, 0.027917942, 0.036297143, -0.04757339, 0.06799607, -0.029136674, 0.0164527, -0.004151858, -0.0031091366, 0.012108401, 0.0057115154, -0.03509417, -0.009540441, 0.011054199, -0.011634528, -0.0005865301, 0.020527013, -0.07506993, 0.056495093, -0.0065905675, -0.006125836, -0.002670125, 0.0054092263, -0.020132588, -0.041221842, -0.035392627, -0.04468168, 0.015628178, 0.05695922, 0.07388791, -0.004000115, -0.026173877, 0.0498964, -0.018561142, 0.019066755, -0.00089629786, -0.013428539, 0.04518621, 0.017250814, -0.020250618, -0.0049887556, 0.017977217, 0.016197301, -0.058792554, 0.013323165, 0.03274083, 0.027183041, 0.0120310355, 0.0089295395, 0.05275658, 0.022277538, 0.01655807, -0.014445818, 0.01162226, -0.044128895, 0.05875574, -0.038738552, 0.0069667725, -0.037731998, 0.024408735, -0.01883984, -0.014331897, -0.025532316, -0.04500283, 0.006616159, 0.023971764, 0.08077317, -0.024688583, 0.031037092, 0.0074626943, 0.017046604, 0.019829012, -0.026550004, -0.0033259483, -0.013534535, -0.0066377097, 0.011104696, -0.05049998, -0.035800613, -0.014730913, 0.08741782, 0.022309268, -0.017210074, -0.024817437, -0.003679103, 0.009624245, -0.032905675, -0.03139828, 0.05175171, 0.029964553, -0.0021999914, -0.064084135, 0.04997618, 0.028509052, 0.048161868, 0.06321754, 0.035021115, -0.077487014, 0.0063456716, 0.0065494375, -0.029103467, 0.011713456, 0.030397411, 0.011976614, -0.06324206, 0.053408287, 0.06769643, -0.014064329, -0.035596367, 0.055032175, -0.012849076, -0.07371634, -0.0437603, -0.051365767, -0.016642122, 0.01551809, -0.025219483, 0.01819951, 0.024738474, -0.020121481, -0.041063514, -0.022345817, 0.04036475, 0.03872804, -0.11717954, 0.033690542, 0.013192776, -0.0022614624, 0.014616389, 0.01876388, -0.07226051, -0.012287661, -0.01134135, 0.046873707, -0.078656234, 0.07460051, 0.0134823015, -0.03332616, 0.049939923, 0.06019229, -0.038081385, 0.0037394946, 0.0057434733, 0.024201632, -0.023278996, -0.05516105, -0.034305993, 0.0024503674, -0.03763395, 0.024795514, 0.058871064, 0.03270135, -0.039430212, -0.03114192, 0.023136113, -0.017695948, -0.048765846, 0.022169778, 0.027043004, -0.017932333, -0.040927984, 0.012848395, -0.01123609, 0.05150977, -0.0007649105, 0.01531192, 0.0033106515, -0.021498756, -0.0074122273, -0.0008492266, -0.013653233, -0.014612806, 0.054639492, 0.05724361, 0.018060345, 0.0012777871, -0.08914354, 0.062329553, -0.030360265, -0.011410866, 0.029580317, 0.002813934, -0.02099278, 0.05294949, 0.010752169, 0.0016853957, -0.0031625559, -0.023482174, -0.029634394, 0.03327546, -0.03997839, -0.00076659967, -0.018123884, -0.02665281, 0.07182438, 0.013992434, -0.024106473, 0.03148389, -0.0667189, 0.02151133, -0.035608437, 0.018263593, -0.016029026, -0.10932681, -0.053636, -0.021964783, 0.036643036, 0.06907165, 0.02914369, -0.010569346, 0.011603342, -0.0020385236, -0.041178964, -0.022108674, -0.019529998, 0.010679884, 0.032059927, 0.05083682, 0.023250435, -0.026090337, -0.07262876, -0.0017595277, 0.01264439, 0.006348968, -0.052400447, -0.004959457, -0.0050718263, 0.045453396, 0.0074742106, 0.031368986, -0.019201403, 0.037799016]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buscando o documento\n",
        "Com isso, conseguimos fazer uma busca com o valor sem√¢ntico do input de usu√°rio e o valor sem√¢ntico de cada documento, a partir da qual o modelo conseguir√° entender qual √© a op√ß√£o que mais se adequa √† busca feita pelo nosso usu√°rio. Para isso contaremos com a ajuda do `numpy`."
      ],
      "metadata": {
        "id": "d_Mx7-msUNHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_passage(user_input, dataframe):\n",
        "  query_embedding = genai.embed_content(model=embedding_model,\n",
        "                                        content=user_input,\n",
        "                                        task_type=\"retrieval_query\")\n",
        "  dot_products = np.dot(np.stack(dataframe['Embeddings']), query_embedding[\"embedding\"])\n",
        "  idx = np.argmax(dot_products)\n",
        "  return dataframe.iloc[idx]['Menu'] # Return text from index with max value"
      ],
      "metadata": {
        "id": "PW1tta-a9T4i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma vez encontrado o trecho que possui maior aproxima√ß√£o com o que foi solicitado pelo usu√°rio, retornamos o menu que tem maior pertin√™ncia com a busca."
      ],
      "metadata": {
        "id": "pJ2f-bAHVAvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_text = find_best_passage(user_input, df)\n",
        "print(result_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "cETQ5hpA9kJG",
        "outputId": "4484ea65-4ef3-41f8-8782-61e39f83db6b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Big Clown\n",
            "\n",
            "    7 euros\n",
            "\n",
            "    90g Hamburguer\n",
            "    Lettuce\n",
            "    Tomatoes\n",
            "    Mayonnaise\n",
            "    Picles\n",
            "    Cheddar\n",
            "    ---\n",
            "    Fish Clown Burguer\n",
            "\n",
            "    6 euros\n",
            "\n",
            "    Fish hamburguer\n",
            "    Lettuce\n",
            "    Tomatoes\n",
            "    Mayonnaise\n",
            "    Cheddar\n",
            "    Lemon-based sauce dip\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformando em texto\n",
        "\n",
        "Por fim, agora podemos transformar o resultado dessa busca vetorizada em algo mais humanizado. A vantagem de realizar essa busca a partir do embedding √© que garantimos que a fonte da nossa informa√ß√£o vir√° somente dos dados que fornecemos ao modelo, evitando assim at√© certo ponto alucina√ß√µes provenientes do excesso de informa√ß√µes dispon√≠veis para a IA Generativa.\n",
        "\n",
        "Agora, vamos transformar esse resultado em uma resposta mais amig√°vel para o usu√°rio. Para isso, utilizaremos mais uma vez nosso modelo generativo, o Gemini 1.5 Pro.\n",
        "\n",
        "Criaremos uma fun√ß√£o contendo um prompt, com base nos par√¢metros recebidos: o input do usu√°rio e o texto mais relevante de acordo com a busca feita anteriormente usando *embedding*. Pediremos ao modelo que:\n",
        "\n",
        "*   Seja sucinto e foque no que foi perguntado de maneira precisa\n",
        "*   Seja o mais direto e conciso poss√≠vel\n",
        "*   Use dados externos somente para trazer informa√ß√µes nutricionais (que n√£o est√£o contidadas nos documentos fornecidos), poss√≠veis riscos a sa√∫do e restri√ß√µes alimentares relacionadas.\n",
        "*   Se a quest√£o estiver fora do contexto, sinalize educadamente que n√£o pode responder a isso.\n",
        "*   Se a quest√£o estiver dentro do contexto mas sem resultados relevantes o suficiente, informe que n√£o obteve resultados suficientemente significativos.\n"
      ],
      "metadata": {
        "id": "HyEUqswyVOP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the prompt that will be passed to the generative model, in order to create a more friendly and custom response to the user\n",
        "# Returns the prompt, containing the instructions (i.e.: tone), alternative scenarios, the question and the result text from the embedding\n",
        "def create_formatted_prompt(user_input, result_text):\n",
        "  escaped_text = result_text.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
        "  prompt = textwrap.dedent(\"\"\"You're intended to synthetize the result text below, adapting the answer to what has been specifically asked. \\\n",
        "  Make sure the answer is as direct and concise as possible. \\\n",
        "  You can only bring data that is not directly mentioned in the result text if you use it to complement small pieces of information, keeping it short and concise. \\\n",
        "  Scenarios where you can use external data to give an answer: gathering more information about an ingredient (nutrition facts),\n",
        "  potential benefits or harm of each ingredient for human health, diet restrictions related to any ingredient. \\\n",
        "  The answer must be given in the same language as the input question, and mandatorily must have to do with the result text. \\\n",
        "  If the question is out of the context and has anything to do with the result text, apologize and inform you can't answer the question.\n",
        "  If the question is in the context but still has no relevant results, apologize and inform that the question had no relevant results given the presented result text.\n",
        "  QUESTION: '{user_input}'\n",
        "  RESULT TEXT: '{escaped_text}'\n",
        "    ANSWER:\n",
        "  \"\"\").format(user_input=user_input, escaped_text=escaped_text)\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "JvEmvrcA_RRn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver abaixo o prompt pronto:"
      ],
      "metadata": {
        "id": "ZO780SKrWgih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = create_formatted_prompt(user_input, result_text)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GID1SWdAA7qz",
        "outputId": "9107e120-9cc2-4f58-cf9a-22a36e00e1dd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're intended to synthetize the result text below, adapting the answer to what has been specifically asked.   Make sure the answer is as direct and concise as possible.   You can only bring data that is not directly mentioned in the result text if you use it to complement small pieces of information, keeping it short and concise.   Scenarios where you can use external data to give an answer: gathering more information about an ingredient (nutrition facts),\n",
            "  potential benefits or harm of each ingredient for human health, diet restrictions related to any ingredient.   The answer must be given in the same language as the input question.   If the question is out of the context, apologize and inform you can't answer the question.\n",
            "  If the question is in the context but still has no relevant results, apologize and inform that the question had no relevant results given the presented result text. \n",
            "  QUESTION: 'Sugira um prato com alto teor cal√≥rico'\n",
            "  RESULT TEXT: '     Big Clown      7 euros      90g Hamburguer     Lettuce     Tomatoes     Mayonnaise     Picles     Cheddar     ---     Fish Clown Burguer      6 euros      Fish hamburguer     Lettuce     Tomatoes     Mayonnaise     Cheddar     Lemon-based sauce dip     '\n",
            "    ANSWER:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E por, fim, a resposta do modelo para usu√°rio:"
      ],
      "metadata": {
        "id": "bbqm6McAWkgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = model.generate_content(prompt)\n",
        "print(answer.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "60WNCH03BeaB",
        "outputId": "6e981e22-d58e-4dd8-807e-63bea1f6fea9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O Big Clown tem alto teor cal√≥rico por conter hamb√∫rguer, maionese e cheddar.  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclus√£o\n",
        "\n",
        "O projeto passa com √™xito pelas principais funcionalidades do Gemini. Para a tarefa escolhida, n√£o julguei necess√°rio configura√ß√µes mais avan√ßadas, como `temperature`, `top_K`, `top_P` e por isso segui como Gemini 1.5 Pro, utilizando seus valores padr√£o. Em um projeto em que a mudan√ßa desses valores fosse necess√°ria, seria recomendada a utiliza√ß√£o do Gemini 1.0 Pro, que √© um modelo j√° lapidado que possui essas parametriza√ß√µes dispon√≠veis.\n",
        "\n",
        "Concluir este projeto me permitiu identificar aplica√ß√µes pr√°ticas e muito √∫teis da AI em situa√ß√µes reais no mundo em que vivemos hoje. Agrade√ßo √† Alura e √† Google pelos conhecimentos fornecidos durante a imers√£o que me possibilitaram a constru√ß√£o deste pequeno projeto, que possui grande potencial de aplica√ß√£o em produtos que usamos todos os dias para pedir comida, e que possui requisitos t√©cnicos que poderei utilizar em projetos futuros."
      ],
      "metadata": {
        "id": "ccrtVD8yWnQg"
      }
    }
  ]
}